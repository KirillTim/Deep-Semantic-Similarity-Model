{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Michael A. Alcorn (malcorn@redhat.com)\n",
    "# An implementation of the Deep Semantic Similarity Model (DSSM) found in [1].\n",
    "# [1] Shen, Y., He, X., Gao, J., Deng, L., and Mesnil, G. 2014. A latent semantic model\n",
    "#         with convolutional-pooling structure for information retrieval. In CIKM, pp. 101-110.\n",
    "#         http://research.microsoft.com/pubs/226585/cikm2014_cdssm_final.pdf\n",
    "# [2] http://research.microsoft.com/en-us/projects/dssm/\n",
    "# [3] http://research.microsoft.com/pubs/238873/wsdm2015.v3.pdf\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend\n",
    "from keras.layers import Input, merge\n",
    "from keras.layers.core import Dense, Lambda, Reshape\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LETTER_GRAM_SIZE = 3 # See section 3.2.\n",
    "WINDOW_SIZE = 3 # See section 3.2.\n",
    "TOTAL_LETTER_GRAMS = int(3 * 1e4) # Determined from data. See section 3.2.\n",
    "WORD_DEPTH = WINDOW_SIZE * TOTAL_LETTER_GRAMS # See equation (1).\n",
    "K = 300 # Dimensionality of the max-pooling layer. See section 3.4.\n",
    "L = 128 # Dimensionality of latent semantic space. See section 3.5.\n",
    "J = 4 # Number of random unclicked documents serving as negative examples for a query. See section 4.\n",
    "FILTER_LENGTH = 1 # We only consider one time step for convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input tensors holding the query, positive (clicked) document, and negative (unclicked) documents.\n",
    "# The first dimension is None because the queries and documents can vary in length.\n",
    "query = Input(shape = (None, WORD_DEPTH))\n",
    "pos_doc = Input(shape = (None, WORD_DEPTH))\n",
    "neg_docs = [Input(shape = (None, WORD_DEPTH)) for j in range(J)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query model. The paper uses separate neural nets for queries and documents (see section 5.2).\n",
    "\n",
    "# In this step, we transform each word vector with WORD_DEPTH dimensions into its\n",
    "# convolved representation with K dimensions. K is the number of kernels/filters\n",
    "# being used in the operation. Essentially, the operation is taking the dot product\n",
    "# of a single weight matrix (W_c) with each of the word vectors (l_t) from the\n",
    "# query matrix (l_Q), adding a bias vector (b_c), and then applying the tanh function.\n",
    "# That is, h_Q = tanh(W_c • l_Q + b_c). With that being said, that's not actually\n",
    "# how the operation is being calculated here. To tie the weights of the weight\n",
    "# matrix (W_c) together, we have to use a one-dimensional convolutional layer. \n",
    "# Further, we have to transpose our query matrix (l_Q) so that time is the first\n",
    "# dimension rather than the second (as described in the paper). That is, l_Q[0, :]\n",
    "# represents our first word vector rather than l_Q[:, 0]. We can think of the weight\n",
    "# matrix (W_c) as being similarly transposed such that each kernel is a column\n",
    "# of W_c. Therefore, h_Q = tanh(l_Q • W_c + b_c) with l_Q, W_c, and b_c being\n",
    "# the transposes of the matrices described in the paper.\n",
    "query_conv = Convolution1D(K, FILTER_LENGTH, border_mode = \"same\", input_shape = (None, WORD_DEPTH), activation = \"tanh\")(query) # See equation (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, we apply a max-pooling layer to the convolved query matrix. Keras provides\n",
    "# its own max-pooling layers, but they cannot handle variable length input (as\n",
    "# far as I can tell). As a result, I define my own max-pooling layer here. In the\n",
    "# paper, the operation selects the maximum value for each row of h_Q, but, because\n",
    "# we're using the transpose, we're selecting the maximum value for each column.\n",
    "query_max = Lambda(lambda x: backend.max(x, axis = 1), output_shape = (K, ))(query_conv) # See section 3.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this step, we generate the semantic vector represenation of the query. This\n",
    "# is a standard neural network dense layer, i.e., y = tanh(W_s • v + b_s).\n",
    "query_sem = Dense(L, activation = \"tanh\", input_dim = K)(query_max) # See section 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The document equivalent of the above query model.\n",
    "doc_conv = Convolution1D(K, FILTER_LENGTH, border_mode = \"same\", input_shape = (None, WORD_DEPTH), activation = \"tanh\")\n",
    "doc_max = Lambda(lambda x: backend.max(x, axis = 1), output_shape = (K, ))\n",
    "doc_sem = Dense(L, activation = \"tanh\", input_dim = K)\n",
    "\n",
    "pos_doc_conv = doc_conv(pos_doc)\n",
    "neg_doc_convs = [doc_conv(neg_doc) for neg_doc in neg_docs]\n",
    "\n",
    "pos_doc_max = doc_max(pos_doc_conv)\n",
    "neg_doc_maxes = [doc_max(neg_doc_conv) for neg_doc_conv in neg_doc_convs]\n",
    "\n",
    "pos_doc_sem = doc_sem(pos_doc_max)\n",
    "neg_doc_sems = [doc_sem(neg_doc_max) for neg_doc_max in neg_doc_maxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This layer calculates the cosine similarity between the semantic representations of\n",
    "# a query and a document.\n",
    "R_Q_D_p = merge([query_sem, pos_doc_sem], mode = \"cos\") # See equation (4).\n",
    "R_Q_D_ns = [merge([query_sem, neg_doc_sem], mode = \"cos\") for neg_doc_sem in neg_doc_sems] # See equation (4).\n",
    "\n",
    "concat_Rs = merge([R_Q_D_p] + R_Q_D_ns, mode = \"concat\")\n",
    "concat_Rs = Reshape((J + 1, 1))(concat_Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this step, we multiply each R(Q, D) value by gamma. In the paper, gamma is\n",
    "# described as a smoothing factor for the softmax function, and it's set empirically\n",
    "# on a held-out data set. We're going to learn gamma's value by pretending it's\n",
    "# a single 1 x 1 kernel.\n",
    "weight = np.array([1]).reshape(1, 1, 1, 1)\n",
    "with_gamma = Convolution1D(1, 1, border_mode = \"same\", input_shape = (J + 1, 1), activation = \"linear\", bias = False, weights = [weight])(concat_Rs) # See equation (5).\n",
    "\n",
    "# Next, we exponentiate each of the gamma x R(Q, D) values.\n",
    "exponentiated = Lambda(lambda x: backend.exp(x), output_shape = (J + 1, ))(with_gamma) # See equation (5).\n",
    "exponentiated = Reshape((J + 1, ))(exponentiated)\n",
    "\n",
    "# Finally, we use the softmax function to calculate the P(D+|Q).\n",
    "prob = Lambda(lambda x: backend.softmax(x), output_shape = (5, ))(exponentiated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now have everything we need to define our model.\n",
    "model = Model(input = [query, pos_doc] + neg_docs, output = prob)\n",
    "model.compile(optimizer = \"adadelta\", loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a random data set.\n",
    "sample_size = 10\n",
    "l_Qs = []\n",
    "pos_l_Ds = []\n",
    "\n",
    "for i in range(sample_size):\n",
    "    query_len = np.random.randint(1, 10)\n",
    "    l_Q = np.random.rand(1, query_len, WORD_DEPTH)\n",
    "    l_Qs.append(l_Q)\n",
    "    \n",
    "    doc_len = np.random.randint(50, 200)\n",
    "    l_D = np.random.rand(1, doc_len, WORD_DEPTH)\n",
    "    pos_l_Ds.append(l_D)\n",
    "\n",
    "neg_l_Ds = []\n",
    "for i in range(sample_size):\n",
    "    possibilities = list(range(sample_size))\n",
    "    possibilities.remove(i)\n",
    "    negatives = np.random.choice(possibilities, J)\n",
    "    neg_l_Ds.append([pos_l_Ds[negative] for negative in negatives])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ 0.14475264]]], dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because we're using the \"categorical_crossentropy\" loss function, we can pretend that\n",
    "# we're dealing with a multi-class classification problem and that every sample is a\n",
    "# member of the \"0\" class.\n",
    "y = np.zeros(J + 1).reshape(1, J + 1)\n",
    "y[0][0] = 1\n",
    "\n",
    "for i in range(sample_size):\n",
    "    history = model.fit([l_Qs[i], pos_l_Ds[i]] + neg_l_Ds[i], y, nb_epoch = 1, verbose = 0)\n",
    "\n",
    "# Here, I walk through how to define a function for calculating output from the\n",
    "# computational graph. Let's define a function that calculates R(Q, D+) for a given\n",
    "# query and clicked document. The function depends on two inputs, query and pos_doc.\n",
    "# That is, if you start at the point in the graph where R(Q, D+) is calculated\n",
    "# and then work backwards as far as possible, you'll end up at two different starting\n",
    "# points: query and pos_doc. As a result, we supply those inputs in a list to the\n",
    "# function. This particular function only calculates a single output, but multiple\n",
    "# outputs are possible (see the next example).\n",
    "get_R_Q_D_p = backend.function([query, pos_doc], [R_Q_D_p])\n",
    "get_R_Q_D_p([l_Qs[0], pos_l_Ds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ 0.26476961]]], dtype=float32),\n",
       " array([[[ 0.26528367]]], dtype=float32),\n",
       " array([[[ 0.26565051]]], dtype=float32),\n",
       " array([[[ 0.2659288]]], dtype=float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A slightly more complex function. Notice that both neg_docs and the output are\n",
    "# lists.\n",
    "get_R_Q_D_ns = backend.function([query] + neg_docs, R_Q_D_ns)\n",
    "get_R_Q_D_ns([l_Qs[0]] + neg_l_Ds[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
